# 并行算法

## 实验部分

- 对比：PV-Tree 和 baseline algorithms

- 数据集： learning to rank (LTR) 排序学习 和 ad click prediction (CTR 点击率 click-through rate)

    1. 排序学习是在处理排序问题时采用机器学习方法来训练模型的方法  [Learning to Rank(LTR)](https://blog.csdn.net/clheang/article/details/45674989)

    2. CTR（Click-Through-Rate）即点击通过率，是互联网广告常用的术语，指网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率，即该广告的实际点击次数（严格的来说，可以是到达目标页面的数量）除以广告的展现量（Show content） [https://baike.baidu.com/item/CTR/10653699](https://baike.baidu.com/item/CTR/10653699)

![2](http://ww1.sinaimg.cn/large/006alGmrly1g4dng8tmimj30i006qt9f.jpg)

- 衡量方式

    1. NDCG（Normalized discounted cumulative gain）：是用来衡量排序质量的指标 [理解NDCG](https://www.cnblogs.com/HappyAngel/p/3535919.html)

    2. AUC:是一个模型评价指标，只能用于二分类模型的评价 [https://www.zhihu.com/question/39840928](https://www.zhihu.com/question/39840928)

- 根据最近的实践研究，单个决策树很难再复杂任务中获得有效的模型，于是我们的算法会基于 boosting algorithms 来执行任务

    GBDT:Gradient Boosting Decision Tree.是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。 [https://www.jianshu.com/p/005a4e6ac775](https://www.jianshu.com/p/005a4e6ac775)

- 其它算法：

    1. 属性并行算法 attribute-parallel algorithm：二进制向量用于指示拆分信息并在机器之间交换

    2. 数据并行算法：传递全粒度直方图和量化直方图

![2](http://ww1.sinaimg.cn/large/006alGmrly1g4dnkjv0zej30jz06dgmo.jpg)

-实验环境：服务器集群（每个服务器有12个CPU核心和32 GB RAM）与1 Gbps以太网互连。 对于LTR的实验，我们使用8台机器进行并行训练; 对于CTR的实验，我们使用了32台机器，因为数据集要大得多。

根据最近的工业实践，单个决策树可能不够强大，无法学习排序和点击预测等复杂任务的有效模型。 因此，人们通常使用基于决策树的提升算法（例如，GBDT）来执行任务。 在本文中，我们还使用GBDT作为平台来检查决策树并行化的效率和有效性。 也就是说，我们使用PV-Tree或其他基线算法在GBDT的每次迭代中并行化决策树构建过程，并比较它们的性能。 我们的实验环境是一组服务器（每个服务器有12个CPU核心和32 GB RAM）与1 Gbps以太网互连。 对于LTR的实验，我们使用8台机器进行并行训练; 对于CTR的实验，我们使用了32台机器，因为数据集要大得多。

### Comparison with Other Parallel Decision Trees

![2](http://ww1.sinaimg.cn/large/006alGmrly1g4dnnkkw0mj31260d3jwa.jpg)

为了与PV-Tree进行比较，我们实现了一种属性并行算法，其中二进制向量用于指示拆分信息并在机器之间交换。 此外，我们根据[2,21]实现了数据并行算法，它可以传递全粒度直方图和量化直方图。 所有并行算法和顺序（单机）版本都进行了比较

对于LTR，由于数据样本的数量相对较小，因此关于样本的分割信息的通信不会花费太多时间。 结果，属性并行算法似乎是有效的。 由于大多数属性在此数据集中采用数值，因此全粒度直方图具有相当多的二进制数。 因此，传递全粒度直方图的数据并行算法非常慢，甚至比顺序算法慢。 当将直方图中的区间减少到10％时，数据并行算法变得更加有效，但是，其收敛点不好（与我们的理论一致 - 量化直方图中的偏差导致精度下降）。

对于CTR，属性并行算法变得非常慢，因为数据样本的数量非常大。 相比之下，CTR中的许多属性采用二进制或离散值，这使得全粒度直方图具有有限数量的二进制数。 因此，具有全粒度直方图的数据并行算法比顺序算法更快。 具有量化直方图的数据并行算法甚至更快，然而，其收敛点再次不是很好。

PV-Tree在LTR和CTR任务中在最短时间内达到通过顺序算法实现的最佳点。 为了更有效地比较效率，我们列出了每种算法的时间（LTR为8台机器，CTR为32台机器），以达到表2中顺序算法的收敛精度。从表中我们可以看到，对于LTR 它花费了PV-Tree 5825秒，同时它分别计算了数据并行算法（具有全粒度直方图9）和属性并行算法32260和14660秒。 与顺序算法（花费28690秒收敛）相比，PV-Tree在8台机器上的速度提高了4.9倍。 对于CTR，它花费了PV-Tree 5349秒，同时它分别计算了数据并行算法（具有全粒度直方图）和属性并行算法9209和26928秒。 与顺序算法（花费154112秒收敛）相比，PV-Tree在32台机器上的速度提高了28.8倍

我们还进行了独立实验，以便在给出一些典型的大数据工作负载设置的情况下，对不同并行算法的通信成本进 结果列于
表3.我们发现属性并行算法的成本是相对于训练数据N的大小，并且数据并行算法的成本是相对于属性数d的。 相比之下，PV-Tree的成本是不变的

![2](http://ww1.sinaimg.cn/large/006alGmrly1g4dnw25s3wj30hx0djmz2.jpg)

### 5.2 Tradeoff between Speed-up and Accuracy in PV-Tree

在前一小节中，我们已经证明PV树比其他算法更有效。 在这里，我们深入研究PV树，看看它的关键参数如何影响效率之间的权衡
和准确性。 根据定理4.1，以下两个参数对PV-Tree至关重要：机器数量M和投票k的大小

#### 5.2.1 On Different Numbers of Machines

在前一小节中，我们已经证明PV树比其他算法更有效。 在这里，我们深入研究PV树，看看它的关键参数如何影响效率之间的权衡
和准确性。 根据定理4.1，以下两个参数对PV-Tree至关重要：机器数量M和投票k的大小

当更多机器加入分布式培训流程时，数据吞吐量将变得更大，但每台机器上的摊销培训数据将变得更小。 根据我们的定理，当每台机器上的数据量变得太小时，将无法保证投票程序的准确性。 因此，适当设置机器数量非常重要

为了获得更多的见解，我们进行了一些额外的实验，其结果显示在图2a和2b中。 从这些数字中，我们可以看到，对于LTR，当机器的数量
从2增长到8，训练过程显着加快。 但是，当数量达到16时，收敛速度甚至低于使用8台机器的收敛速度。 CTR可以观察到类似的结果。 这些观察结果与我们的理论发现一致。 请注意，PV-Tree专为大数据场景而设计。 只有当整个训练数据很大（并且因此每个本地机器上的训练数据的分布可以类似于整个训练数据的分布）时，才能实现PV树的全部功率。 否则，我们需要对加速有合理的期望，并且应该选择使用较少数量的机器来并行化培训。

![2](http://ww1.sinaimg.cn/large/006alGmrly1g4dp11aduqj312g0b677y.jpg)

#### 5.2.2 On Different Sizes of Voting

在PV-Tree中，我们有一个参数k，它控制在本地和全局投票期间选择的顶级属性的数量。 直观地说，较大的k将增加从本地候选者中找到全局最佳属性的概率，然而，这也意味着更高的通信成本。 根据我们的定理，k的选择应该取决于当地训练数据的大小。 如果本地训练数据的大小很大，则本地最佳属性将类似于全局最佳属性。 在这种情况下，可以安全地选择小的k值。 否则，我们应该选择一个相对较大的k。 为了获得更多的见解，我们进行了一些实验，其结果如表4所示，其中M指的是机器数量。 从表中，我们有以下观察。 首先，对于这两种情况，为了获得良好的准确性，不需要选择大的k。 当k≤40时，精度非常好。 其次，我们发现对于使用少量机器的情况，k可以设置为更小的值，例如，k = 5.这是因为，给定固定大小的训练数据，当使用更少的机器时，大小 每台机器的训练数据将变得更大，因此较小的k已经可以保证近似精度

### 5.3 Comparison with Other Parallel GBDT Algorithms

![2](http://ww1.sinaimg.cn/large/006alGmrly1g4dq21op6mj310n0deaeb.jpg)

虽然我们主要关注如何在前面的小节中并行化GBDT中的决策树构建过程，但也可以通过其他方式并行化GBDT。例如，在[22,20]中，
每台机器在没有通信的情况下分别学习自己的决策树。之后，这些决策树通过赢家通吃或输出集合进行聚合。虽然这些作品不是我们论文的重点，但与它们进行比较仍然很有意思。为此，我们实现了[22]和[20]中提出的算法。为便于参考，我们将它们分别表示为Svore和Yu。它们的性能如图3a和3b所示。从图中可以看出，PV-Tree优于Svore和Yu：尽管这两种算法以与PV-Tree类似的速度收敛，但它们的收敛点要差得多。根据我们有限的理解，这两种算法缺乏扎实的理论保证。由于候选决策树是在没有必要的信息交换的情况下单独和独立地进行训练的，因此它们可能具有不可忽略的偏差，这将导致最终的准确度下降。相反，我们可以清楚地表征PV树的理论属性，并在适当的设置中使用它，以避免可观察到的精度下降。总结所有实验，我们可以看到，通过适当设置的参数，PV-Tree可以在效率和准确度之间取得很好的平衡，并且优于专为GBDT并行化设计的其他并行决策树算法。
